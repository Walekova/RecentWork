{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Hadoop MR\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2020`__\n",
    "\n",
    "We use Hadoop MapReduce to implement simple parallelized machine learning algorithm: Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars (paths) - ADJUST AS NEEDED\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "HDFS_DIR = \"/user/root/HW2\"\n",
    "HOME_DIR = \"\" # FILL IN HERE eg. /media/notebooks/Assignments/HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "ENRON = \"data/enronemail_1h.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2020-01-25 16:02 demo2\n"
     ]
    }
   ],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -ls \n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/Assignments/HW2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n",
      "0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n",
      "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n",
      "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n",
      "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n",
    "!head -n 5 /media/notebooks/Assignments/HW2/{ENRON} | cut -c-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 /media/notebooks/Assignments/HW2/data/enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "# see how many messages/lines are in the file \n",
    "#(this number may be off by 1 if the last line doesn't end with a newline)\n",
    "!wc -l /media/notebooks/Assignments/HW2/{ENRON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/root/HW2': File exists\n"
     ]
    }
   ],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal /media/notebooks/Assignments/HW2/{ENRON} {HDFS_DIR}/enron.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup     204559 2020-01-26 11:22 /user/root/HW2/enron.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Ham/Spam EDA.\n",
    "Before building the classifier, lets get aquainted with our data. We're interested in which words occur more in spam emails than in legitimate (\"ham\") emails. \n",
    "\n",
    "We implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n",
    "!chmod a+x EnronEDA/mapper.py\n",
    "!chmod a+x EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob7889522164996968704.jar tmpDir=null\n",
      "20/01/26 15:05:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/26 15:05:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/26 15:05:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/26 15:05:55 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/26 15:05:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0027\n",
      "20/01/26 15:05:55 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0027\n",
      "20/01/26 15:05:55 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0027/\n",
      "20/01/26 15:05:55 INFO mapreduce.Job: Running job: job_1579903014542_0027\n",
      "20/01/26 15:06:02 INFO mapreduce.Job: Job job_1579903014542_0027 running in uber mode : false\n",
      "20/01/26 15:06:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/26 15:06:10 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/26 15:06:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/26 15:06:19 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "20/01/26 15:06:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/26 15:06:21 INFO mapreduce.Job: Job job_1579903014542_0027 completed successfully\n",
      "20/01/26 15:06:21 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369010\n",
      "\t\tFILE: Number of bytes written=1335626\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217061\n",
      "\t\tHDFS: Number of bytes written=119173\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9979\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10673\n",
      "\t\tTotal time spent by all map tasks (ms)=9979\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10673\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9979\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10673\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10218496\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10929152\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=369022\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=369022\n",
      "\t\tReduce input records=31490\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=62980\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=267\n",
      "\t\tCPU time spent (ms)=8530\n",
      "\t\tPhysical memory (bytes) snapshot=1146114048\n",
      "\t\tVirtual memory (bytes) snapshot=5502324736\n",
      "\t\tTotal committed heap usage (bytes)=1195376640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216847\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=119173\n",
      "20/01/26 15:06:21 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t1\t8\n",
      "assistance\t0\t2\n"
     ]
    }
   ],
   "source": [
    "# part b - write your grep command here\n",
    "!grep 'assistance' EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob8844326108999041629.jar tmpDir=null\n",
      "20/01/26 15:36:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/26 15:36:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/26 15:36:05 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "20/01/26 15:36:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/26 15:36:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0031\n",
      "20/01/26 15:36:06 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0031\n",
      "20/01/26 15:36:06 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0031/\n",
      "20/01/26 15:36:06 INFO mapreduce.Job: Running job: job_1579903014542_0031\n",
      "20/01/26 15:36:13 INFO mapreduce.Job: Job job_1579903014542_0031 running in uber mode : false\n",
      "20/01/26 15:36:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/26 15:36:20 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/26 15:36:21 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/26 15:36:28 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "20/01/26 15:36:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/26 15:36:29 INFO mapreduce.Job: Job job_1579903014542_0031 completed successfully\n",
      "20/01/26 15:36:29 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149575\n",
      "\t\tFILE: Number of bytes written=893620\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=119411\n",
      "\t\tHDFS: Number of bytes written=129303\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10477\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10057\n",
      "\t\tTotal time spent by all map tasks (ms)=10477\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10057\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10477\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10057\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10728448\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10298368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10130\n",
      "\t\tMap output records=10130\n",
      "\t\tMap output bytes=129303\n",
      "\t\tMap output materialized bytes=149587\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10130\n",
      "\t\tReduce shuffle bytes=149587\n",
      "\t\tReduce input records=10130\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=20260\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=266\n",
      "\t\tCPU time spent (ms)=7900\n",
      "\t\tPhysical memory (bytes) snapshot=1084944384\n",
      "\t\tVirtual memory (bytes) snapshot=5495418880\n",
      "\t\tTotal committed heap usage (bytes)=1195376640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=119173\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129303\n",
      "20/01/26 15:36:29 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d - write your Hadoop streaming job here\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k3,3nr\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "and\t1\t392\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "for\t1\t204\t\n",
      "it\t1\t152\t\n",
      "that\t1\t145\t\n",
      "on\t1\t136\t\n",
      "is\t1\t135\t\n",
      "will\t1\t102\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "your\t1\t357\t\n",
      "in\t1\t236\t\n",
      "com\t1\t153\t\n",
      "this\t1\t143\t\n",
      "i\t1\t140\t\n",
      "or\t1\t117\t\n",
      "we\t1\t116\t\n",
      "with\t1\t116\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part d - view the top 10 records from each partition (RUN THIS CELL AS IS)\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob3006420551311855225.jar tmpDir=null\n",
      "20/01/26 15:31:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/26 15:31:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/26 15:31:10 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "20/01/26 15:31:10 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/26 15:31:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0029\n",
      "20/01/26 15:31:11 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0029\n",
      "20/01/26 15:31:11 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0029/\n",
      "20/01/26 15:31:11 INFO mapreduce.Job: Running job: job_1579903014542_0029\n",
      "20/01/26 15:31:19 INFO mapreduce.Job: Job job_1579903014542_0029 running in uber mode : false\n",
      "20/01/26 15:31:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/26 15:31:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/26 15:31:34 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "20/01/26 15:31:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/26 15:31:36 INFO mapreduce.Job: Job job_1579903014542_0029 completed successfully\n",
      "20/01/26 15:31:36 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149575\n",
      "\t\tFILE: Number of bytes written=894304\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=119411\n",
      "\t\tHDFS: Number of bytes written=129303\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9849\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9573\n",
      "\t\tTotal time spent by all map tasks (ms)=9849\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9573\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9849\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9573\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10085376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9802752\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10130\n",
      "\t\tMap output records=10130\n",
      "\t\tMap output bytes=129303\n",
      "\t\tMap output materialized bytes=149587\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10130\n",
      "\t\tReduce shuffle bytes=149587\n",
      "\t\tReduce input records=10130\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=20260\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=263\n",
      "\t\tCPU time spent (ms)=7530\n",
      "\t\tPhysical memory (bytes) snapshot=1093324800\n",
      "\t\tVirtual memory (bytes) snapshot=5495746560\n",
      "\t\tTotal committed heap usage (bytes)=1195376640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=119173\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129303\n",
      "20/01/26 15:31:36 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadoop streaming job here\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "the\t0\t549\t\n",
      "to\t0\t398\t\n",
      "ect\t0\t382\t\n",
      "and\t0\t278\t\n",
      "of\t0\t230\t\n",
      "hou\t0\t206\t\n",
      "a\t0\t196\t\n",
      "in\t0\t182\t\n",
      "for\t0\t170\t\n",
      "on\t0\t135\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "and\t1\t392\t\n",
      "your\t1\t357\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "in\t1\t236\t\n",
      "for\t1\t204\t\n",
      "com\t1\t153\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part e - view the top 10 records from each partition (RUN THIS CELL AS IS)\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__\n",
    "<table>\n",
    "<th>part-00000:</th>\n",
    "<th>part-00001:</th>\n",
    "<tr><td><pre>\n",
    "the\t0\t549\t\n",
    "to\t0\t398\t\n",
    "ect\t0\t382\t\n",
    "and\t0\t278\t\n",
    "of\t0\t230\t\n",
    "hou\t0\t206\t\n",
    "a\t0\t196\t\n",
    "in\t0\t182\t\n",
    "for\t0\t170\t\n",
    "on\t0\t135\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1\t698\t\n",
    "to\t1\t566\t\n",
    "and\t1\t392\t\n",
    "your\t1\t357\t\n",
    "a\t1\t347\t\n",
    "you\t1\t345\t\n",
    "of\t1\t336\t\n",
    "in\t1\t236\t\n",
    "for\t1\t204\t\n",
    "com\t1\t153\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Ham/Spam NB Classifier & Results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test/Train split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - test/train split (RUN THIS CELL AS IS)\n",
    "!head -n 80 data/enronemail_1h.txt > data/enron_train.txt\n",
    "!tail -n 20 data/enronemail_1h.txt > data/enron_test.txt\n",
    "!hdfs dfs -copyFromLocal data/enron_train.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal data/enron_test.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _without smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob1738499756421908179.jar tmpDir=null\n",
      "20/01/30 22:43:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/30 22:43:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/30 22:43:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/30 22:43:42 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/30 22:43:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0111\n",
      "20/01/30 22:43:42 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0111\n",
      "20/01/30 22:43:42 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0111/\n",
      "20/01/30 22:43:42 INFO mapreduce.Job: Running job: job_1579903014542_0111\n",
      "20/01/30 22:43:49 INFO mapreduce.Job: Job job_1579903014542_0111 running in uber mode : false\n",
      "20/01/30 22:43:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/30 22:43:57 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/30 22:43:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/30 22:44:04 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "20/01/30 22:44:05 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/30 22:44:05 INFO mapreduce.Job: Job job_1579903014542_0111 completed successfully\n",
      "20/01/30 22:44:05 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=368689\n",
      "\t\tFILE: Number of bytes written=1339044\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=179539\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10584\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10006\n",
      "\t\tTotal time spent by all map tasks (ms)=10584\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10006\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10584\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10006\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10838016\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10246144\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25073\n",
      "\t\tMap output bytes=318531\n",
      "\t\tMap output materialized bytes=368701\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4558\n",
      "\t\tReduce shuffle bytes=368701\n",
      "\t\tReduce input records=25073\n",
      "\t\tReduce output records=4556\n",
      "\t\tSpilled Records=50146\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=309\n",
      "\t\tCPU time spent (ms)=9010\n",
      "\t\tPhysical memory (bytes) snapshot=1143336960\n",
      "\t\tVirtual memory (bytes) snapshot=5523619840\n",
      "\t\tTotal committed heap usage (bytes)=1195376640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=179539\n",
      "20/01/30 22:44:05 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-model\n",
      "mkdir: cannot create directory `NaiveBayes/Unsmoothed': File exists\n"
     ]
    }
   ],
   "source": [
    "# part b -  Unsmoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2 \" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/enron-model \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 2\n",
    "\n",
    "# save the model locally\n",
    "!mkdir NaiveBayes/Unsmoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-model/part-000* > NaiveBayes/Unsmoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0001725476662928134,0.00029682398337785694\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000172547666293,0.000296823983378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,8.62738331464067e-05,0.001632531908578213\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,8.62738331464e-05,0.00163253190858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _with Laplace +1 smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob5053958865615182082.jar tmpDir=null\n",
      "20/01/31 00:47:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/31 00:47:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/31 00:47:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/31 00:47:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/31 00:47:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0120\n",
      "20/01/31 00:47:15 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0120\n",
      "20/01/31 00:47:15 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0120/\n",
      "20/01/31 00:47:15 INFO mapreduce.Job: Running job: job_1579903014542_0120\n",
      "20/01/31 00:47:23 INFO mapreduce.Job: Job job_1579903014542_0120 running in uber mode : false\n",
      "20/01/31 00:47:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/31 00:47:31 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/31 00:47:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/31 00:47:39 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "20/01/31 00:47:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/31 00:47:40 INFO mapreduce.Job: Job job_1579903014542_0120 completed successfully\n",
      "20/01/31 00:47:40 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=368689\n",
      "\t\tFILE: Number of bytes written=1339188\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=72217\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10351\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10058\n",
      "\t\tTotal time spent by all map tasks (ms)=10351\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10058\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10351\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10058\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10599424\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10299392\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25073\n",
      "\t\tMap output bytes=318531\n",
      "\t\tMap output materialized bytes=368701\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4558\n",
      "\t\tReduce shuffle bytes=368701\n",
      "\t\tReduce input records=25073\n",
      "\t\tReduce output records=4560\n",
      "\t\tSpilled Records=50146\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=319\n",
      "\t\tCPU time spent (ms)=8880\n",
      "\t\tPhysical memory (bytes) snapshot=1089015808\n",
      "\t\tVirtual memory (bytes) snapshot=5492166656\n",
      "\t\tTotal committed heap usage (bytes)=1195376640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=72217\n",
      "20/01/31 00:47:40 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model\n"
     ]
    }
   ],
   "source": [
    "# part b -  Smoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2 \" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/smooth-model \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "  -numReduceTasks 2\n",
    "\n",
    "# apply POST PROCESS and save the model locally\n",
    "!mkdir NaiveBayes/Smoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model/part-000* | sort | NaiveBayes/smooth_postprocess.py> NaiveBayes/Smoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0001858045336306206,0.00027730020520215184\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000185804533631,0.000277300205202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,0.0001238696890870804,0.0012755809439298986\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,0.000123869689087,0.00127558094393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - write your code in NaiveBayes/evaluation_reducer.py then RUN THIS\n",
    "!chmod a+x NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t6.134092622766479\t7.532326167942548\t1\n",
      "d6\t1\t3.0081547935545476\t3.6041382256608454\t1\n",
      "d7\t0\t3.819085009771877\t6.93634273583425\t1\n",
      "d8\t0\t1.6218604324346575\t4.990432586777937\t1\n",
      "d5\t1\t6.134092622766479\t7.532326167942548\t True\n",
      "d6\t1\t3.0081547935545476\t3.6041382256608454\t True\n",
      "d7\t0\t3.819085009771877\t6.93634273583425\t False\n",
      "d8\t0\t1.6218604324346575\t4.990432586777937\t False\n",
      "# Documents: \t4.0\n",
      "True Positives:\t2.0\n",
      "True Negatives:\t0.0\n",
      "False Positives:\t2.0\n",
      "False Negatives:\t0.0\n",
      "Accuracy:\t0.5\n",
      "Precision:\t0.5\n",
      "Recall:\t1.0\n",
      "F-Score:\t0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test your evaluation job on the chinese model (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py \n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/unsmooth-results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob1169574045150028489.jar tmpDir=null\n",
      "20/01/31 21:37:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/31 21:37:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/31 21:37:17 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/31 21:37:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/31 21:37:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0147\n",
      "20/01/31 21:37:18 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0147\n",
      "20/01/31 21:37:18 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0147/\n",
      "20/01/31 21:37:18 INFO mapreduce.Job: Running job: job_1579903014542_0147\n",
      "20/01/31 21:37:25 INFO mapreduce.Job: Job job_1579903014542_0147 running in uber mode : false\n",
      "20/01/31 21:37:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/31 21:37:33 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/31 21:37:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/31 21:37:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/31 21:37:39 INFO mapreduce.Job: Job job_1579903014542_0147 completed successfully\n",
      "20/01/31 21:37:39 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1348\n",
      "\t\tFILE: Number of bytes written=452284\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49909\n",
      "\t\tHDFS: Number of bytes written=1603\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9493\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3571\n",
      "\t\tTotal time spent by all map tasks (ms)=9493\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3571\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9493\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3571\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9720832\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3656704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=1302\n",
      "\t\tMap output materialized bytes=1354\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1354\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=180\n",
      "\t\tCPU time spent (ms)=3260\n",
      "\t\tPhysical memory (bytes) snapshot=944799744\n",
      "\t\tVirtual memory (bytes) snapshot=4143616000\n",
      "\t\tTotal committed heap usage (bytes)=896532480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1603\n",
      "20/01/31 21:37:39 INFO streaming.StreamJob: Output directory: /user/root/HW2/unsmooth-results\n",
      "0015.1999-12-15.farmer     0                    -580.1213669207012   -231.89378751648943  False\n",
      "0015.2000-06-09.lokay      0                    -92.46802473248029   -87.72182952071803   False\n",
      "0015.2001-02-12.kitchen    0                    -4049.2546523442115  -3211.996354055393   False\n",
      "0015.2001-07-05.sa_and_hp  1                    -581.4450660275342   -651.0936992111352   False\n",
      "0015.2003-12-19.gp         1                    -684.9988948019626   -763.3433463739273   False\n",
      "0016.1999-12-15.farmer     0                    -574.2348663546682   -536.6603366453603   False\n",
      "0016.2001-02-12.kitchen    0                    -857.185157466994    -466.73027759702853  False\n",
      "0016.2001-07-05.sa_and_hp  1                    -581.4450660275342   -651.0936992111352   False\n",
      "0016.2001-07-06.sa_and_hp  1                    -11454.27950342863   -14776.8327442712    False\n",
      "0016.2003-12-19.gp         1                    -265.9306236324974   -749.4645716867724   False\n",
      "0016.2004-08-01.bg         1                    -454.28256634938356  -471.8332266269964   False\n",
      "0017.1999-12-14.kaminski   0                    -283.1721698756806   -250.94400957103468  False\n",
      "0017.2000-01-17.beck       0                    -2607.2310518590175  -1647.1655682358182  False\n",
      "0017.2001-04-03.williams   0                    -366.54672899898736  -343.9697962152703   False\n",
      "0017.2003-12-18.gp         1                    -153.99866436037206  -143.85456975799642  True\n",
      "0017.2004-08-01.bg         1                    -62.783833682335384  -125.39265772970472  False\n",
      "0017.2004-08-02.bg         1                    -1591.520199674238   -1590.9826818230717  True\n",
      "0018.1999-12-14.kaminski   0                    -774.1659241126549   -753.6503702525944   False\n",
      "0018.2001-07-13.sa_and_hp  1                    -2282.8406171897864  -2537.777319238778   False\n",
      "0018.2003-12-18.gp         1                    -2224.298189538506   -2318.4409345541712  False\n",
      "#                          Documents:           20.0\n",
      "True                       Positives:           2.0\n",
      "True                       Negatives:           0.0\n",
      "False                      Positives:           9.0\n",
      "False                      Negatives:           9.0\n",
      "Accuracy:                  0.1\n",
      "Precision:                 0.18181818181818182\n",
      "Recall:                    0.18181818181818182\n",
      "F-Score:                   0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the UNSMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/unsmooth-results\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/Unsmoothed/NBmodel.txt,NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/unsmooth-results \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/unsmooth-results/part-000* > NaiveBayes/Unsmoothed/results.txt\n",
    "!cat NaiveBayes/Unsmoothed/results.txt | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.16.2.jar] /tmp/streamjob4751150189798274221.jar tmpDir=null\n",
      "20/01/31 21:38:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/31 21:38:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/31 21:38:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/01/31 21:38:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/31 21:38:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579903014542_0148\n",
      "20/01/31 21:38:04 INFO impl.YarnClientImpl: Submitted application application_1579903014542_0148\n",
      "20/01/31 21:38:04 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1579903014542_0148/\n",
      "20/01/31 21:38:04 INFO mapreduce.Job: Running job: job_1579903014542_0148\n",
      "20/01/31 21:38:12 INFO mapreduce.Job: Job job_1579903014542_0148 running in uber mode : false\n",
      "20/01/31 21:38:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/31 21:38:19 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/01/31 21:38:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/31 21:38:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/31 21:38:25 INFO mapreduce.Job: Job job_1579903014542_0148 completed successfully\n",
      "20/01/31 21:38:25 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1349\n",
      "\t\tFILE: Number of bytes written=452274\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49909\n",
      "\t\tHDFS: Number of bytes written=1559\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9426\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3584\n",
      "\t\tTotal time spent by all map tasks (ms)=9426\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3584\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9426\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3584\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9652224\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3670016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=1303\n",
      "\t\tMap output materialized bytes=1355\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1355\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=164\n",
      "\t\tCPU time spent (ms)=3070\n",
      "\t\tPhysical memory (bytes) snapshot=889569280\n",
      "\t\tVirtual memory (bytes) snapshot=4129468416\n",
      "\t\tTotal committed heap usage (bytes)=834142208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1559\n",
      "20/01/31 21:38:25 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-results\n",
      "0015.1999-12-15.farmer     0                   -586.4621392860905   -687.177606111272    True\n",
      "0015.2000-06-09.lokay      0                   -105.91959650553648  -109.59741528110919  True\n",
      "0015.2001-02-12.kitchen    0                   -4393.999711044562   -4603.995494568116   True\n",
      "0015.2001-07-05.sa_and_hp  1                   -754.2018818313297   -717.6587578937467   True\n",
      "0015.2003-12-19.gp         1                   -858.6592864616765   -803.3692067894757   True\n",
      "0016.1999-12-15.farmer     0                   -660.4196466548741   -684.6464593421547   True\n",
      "0016.2001-02-12.kitchen    0                   -916.8493891493322   -1024.1623712916742  True\n",
      "0016.2001-07-05.sa_and_hp  1                   -754.2018818313297   -717.6587578937467   True\n",
      "0016.2001-07-06.sa_and_hp  1                   -17058.138378145588  -15513.697974159779  True\n",
      "0016.2003-12-19.gp         1                   -785.6398309841212   -738.2611600431703   True\n",
      "0016.2004-08-01.bg         1                   -553.1934628540757   -515.6366681288395   True\n",
      "0017.1999-12-14.kaminski   0                   -340.6605712605845   -338.5172566018606   False\n",
      "0017.2000-01-17.beck       0                   -2692.280653507079   -3217.4799014436835  True\n",
      "0017.2001-04-03.williams   0                   -395.67884379605795  -392.46028248308914  False\n",
      "0017.2003-12-18.gp         1                   -187.40153738728694  -178.2143152661608   True\n",
      "0017.2004-08-01.bg         1                   -141.75276113075697  -135.18391808144366  True\n",
      "0017.2004-08-02.bg         1                   -1911.1079177336599  -1878.783561563328   True\n",
      "0018.1999-12-14.kaminski   0                   -850.1436521741396   -835.3708160385128   False\n",
      "0018.2001-07-13.sa_and_hp  1                   -2883.8017836350487  -2769.4134435258534  True\n",
      "0018.2003-12-18.gp         1                   -2754.180854550275   -2687.6281424016884  True\n",
      "#                          Documents:          20.0\n",
      "True                       Positives:          11.0\n",
      "True                       Negatives:          6.0\n",
      "False                      Positives:          3.0\n",
      "False                      Negatives:          0.0\n",
      "Accuracy:                  0.85\n",
      "Precision:                 0.7857142857142857\n",
      "Recall:                    1.0\n",
      "F-Score:                   0.88\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the SMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-results\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/Smoothed/NBmodel.txt,NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/smooth-results \\\n",
    "  -cmdenv PATH={PATH} \\\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-results/part-000* > NaiveBayes/Smoothed/results.txt\n",
    "!cat NaiveBayes/Smoothed/results.txt | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== UNSMOOTHED MODEL ============\n",
      "# Documents: \t20.0\n",
      "True Positives:\t2.0\n",
      "True Negatives:\t0.0\n",
      "False Positives:\t9.0\n",
      "False Negatives:\t9.0\n",
      "Accuracy:\t0.1\n",
      "Precision:\t0.18181818181818182\n",
      "Recall:\t0.18181818181818182\n",
      "F-Score:\t0.18181818181818182\n",
      "=========== SMOOTHED MODEL ============\n",
      "# Documents: \t20.0\n",
      "True Positives:\t11.0\n",
      "True Negatives:\t6.0\n",
      "False Positives:\t3.0\n",
      "False Negatives:\t0.0\n",
      "Accuracy:\t0.85\n",
      "Precision:\t0.7857142857142857\n",
      "Recall:\t1.0\n",
      "F-Score:\t0.88\n"
     ]
    }
   ],
   "source": [
    "# part c - display results \n",
    "# NOTE: feel free to modify the tail commands to match the format of your results file\n",
    "print('=========== UNSMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Unsmoothed/results.txt\n",
    "print('=========== SMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
